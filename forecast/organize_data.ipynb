{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc32f28d09cdb7ad",
   "metadata": {},
   "source": [
    "# Notebook to update data for the forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2373bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T13:35:39.111846549Z",
     "start_time": "2024-01-20T13:35:39.049818433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/eduardoaraujo/Documents/Github/transfer-learning-forecast/forecast'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from epiweeks import Week\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e23d81",
   "metadata": {},
   "source": [
    "## LSTM models for the states and \"Macroregionais de saúde\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1117c9",
   "metadata": {},
   "source": [
    "### Path where the cases and climate data are saved:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1340a1f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T13:35:42.316517657Z",
     "start_time": "2024-01-20T13:35:42.291252038Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9869cac4f25a3760",
   "metadata": {},
   "source": [
    "### The dataframe below will be used in the functions to get the link between the geocodes and the health macroregion code: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee86217c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T13:35:44.241158642Z",
     "start_time": "2024-01-20T13:35:44.226870731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geocode</th>\n",
       "      <th>name_muni</th>\n",
       "      <th>name_region</th>\n",
       "      <th>code_region</th>\n",
       "      <th>name_macro</th>\n",
       "      <th>code_macro</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100015</td>\n",
       "      <td>Alta Floresta D'Oeste</td>\n",
       "      <td>Zona da Mata</td>\n",
       "      <td>11005</td>\n",
       "      <td>Cacoal</td>\n",
       "      <td>1101</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100023</td>\n",
       "      <td>Ariquemes</td>\n",
       "      <td>Vale do Jamari</td>\n",
       "      <td>11001</td>\n",
       "      <td>Porto Velho</td>\n",
       "      <td>1102</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1100031</td>\n",
       "      <td>Cabixi</td>\n",
       "      <td>Cone Sul</td>\n",
       "      <td>11006</td>\n",
       "      <td>Cacoal</td>\n",
       "      <td>1101</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100049</td>\n",
       "      <td>Cacoal</td>\n",
       "      <td>Café</td>\n",
       "      <td>11002</td>\n",
       "      <td>Cacoal</td>\n",
       "      <td>1101</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1100056</td>\n",
       "      <td>Cerejeiras</td>\n",
       "      <td>Cone Sul</td>\n",
       "      <td>11006</td>\n",
       "      <td>Cacoal</td>\n",
       "      <td>1101</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geocode              name_muni     name_region  code_region   name_macro  \\\n",
       "0  1100015  Alta Floresta D'Oeste    Zona da Mata        11005       Cacoal   \n",
       "1  1100023              Ariquemes  Vale do Jamari        11001  Porto Velho   \n",
       "2  1100031                 Cabixi        Cone Sul        11006       Cacoal   \n",
       "3  1100049                 Cacoal            Café        11002       Cacoal   \n",
       "4  1100056             Cerejeiras        Cone Sul        11006       Cacoal   \n",
       "\n",
       "   code_macro state  \n",
       "0        1101    RO  \n",
       "1        1102    RO  \n",
       "2        1101    RO  \n",
       "3        1101    RO  \n",
       "4        1101    RO  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = pd.read_csv('../macro_saude.csv')\n",
    "\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a41c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_columns(df):\n",
    "    '''\n",
    "    This function add the number of the ep week, the number of the month and the first difference of the cases \n",
    "    as new columns in the table\n",
    "    '''\n",
    "    \n",
    "    df['month'] = df.index.month\n",
    "    \n",
    "    weeks = []\n",
    "    for date in df.index:\n",
    "        #print(date)\n",
    "        weeks.append(Week.fromdate(date).weektuple()[1])\n",
    "        #print(Week.fromdate(date).weektuple()[1])\n",
    "        #break  \n",
    "        \n",
    "    df['SE'] = weeks\n",
    "    \n",
    "    df.loc[df.index == '2018-04-04', 'SE'] = 15\n",
    "     \n",
    "    diff_series = [df]\n",
    "        \n",
    "    for i in df.columns[df.columns.str.startswith('casos')]:\n",
    "\n",
    "        diff_series.append(pd.DataFrame(data = np.diff(df[f'{i}'], 1), index = df.index[1:], columns = [f'diff_{i}']))\n",
    "\n",
    "    df = pd.concat(diff_series, axis = 1, join = 'outer')    \n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f57de50d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T13:35:46.101773044Z",
     "start_time": "2024-01-20T13:35:46.099286404Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_geocodes_and_state(macro): \n",
    "    '''\n",
    "    This function is used to get the geocodes and state that refer to a specific health macro region code\n",
    "    \n",
    "    :param macro:int. A four-digit number\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    dfs = pd.read_csv('../macro_saude.csv')\n",
    "    \n",
    "    geocodes = dfs.loc[dfs.code_macro == macro].geocode.unique()\n",
    "    state = dfs.loc[dfs.code_macro == macro].state.values[0]\n",
    "\n",
    "    return geocodes, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d47b5fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T13:35:47.375031361Z",
     "start_time": "2024-01-20T13:35:47.372389988Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_geocodes(geocodes):\n",
    "    \n",
    "    '''\n",
    "    This function split the geocodes between the cities with populations up and below 30k in 2022.\n",
    "    \n",
    "    :param geocode:list of int. A list with seven-digit ibge codes for brazilian cities \n",
    "     \n",
    "    '''\n",
    "    \n",
    "    dfpop = pd.read_csv('poptcu2010-2022_rgi.csv')\n",
    "\n",
    "    g_low = dfpop.loc[ (dfpop.CODMUN7.isin(geocodes)) & (dfpop.POP22 <= 30000) ].CODMUN7.unique()\n",
    "    \n",
    "    g_up = np.setdiff1d(geocodes, g_low)\n",
    "    \n",
    "    if geocodes.shape[0] != g_low.shape[0] + g_up.shape[0]:\n",
    "    \n",
    "        print('Error subtracting geocodes')\n",
    "    \n",
    "    return g_up, g_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea8e08df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T13:35:48.680252471Z",
     "start_time": "2024-01-20T13:35:48.674440994Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(df, geocode, geo_col = 'municipio_geocodigo'): \n",
    "    '''\n",
    "    This filters the data for a specific region and returns it as a separate dataframe.\n",
    "    \n",
    "    :param df: pd.DataFrame.\n",
    "    :param geocode:. Must be at the same type of the geo_col \n",
    "    :param geo_col: str. Name of the column in the df that it will be used to filter the geocode value\n",
    "     \n",
    "    '''\n",
    "        \n",
    "    \n",
    "    df_ep = df.loc[df[geo_col] == geocode]\n",
    "    \n",
    "    del df_ep[geo_col]\n",
    "    \n",
    "    df_ep.columns = df_ep.columns + f'_{geocode}'\n",
    "    \n",
    "    return df_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "115cd145",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T13:35:50.765132564Z",
     "start_time": "2024-01-20T13:35:50.721561865Z"
    }
   },
   "outputs": [],
   "source": [
    "predictors_clim = ['temp_min', 'temp_max', 'umid_min', 'umid_max',\n",
    "                   'pressao_min', 'pressao_max', 'precip_tot', 'rainy_days',\n",
    "                   'temp_mean', 'temp_amp','umid_mean','umid_amp',\n",
    "                   'pressao_mean']\n",
    "\n",
    "def predictors_ep_macro(macro): \n",
    "    '''\n",
    "    This function is used to organize in a table the epidemiological predictors related to a specific health macroregion\n",
    "    \n",
    "    :params macro: int. A four digit number\n",
    "    '''\n",
    "    \n",
    "    geocodes, state = get_geocodes_and_state(macro)\n",
    "\n",
    "    # get epidemiological factors \n",
    "    df_ep = pd.read_parquet(f'{PATH}/cases/{state}_dengue.parquet',\n",
    "                           columns = ['data_iniSE', 'casos_est', 'municipio_geocodigo', 'p_rt1', 'Rt', 'p_inc100k'])\n",
    "    \n",
    "    # select only the geocodes include in the health macroregion\n",
    "    df_ep = df_ep.loc[df_ep.municipio_geocodigo.isin(geocodes)]\n",
    "    \n",
    "    df_ep = df_ep.sort_index()\n",
    "    \n",
    "    # split the geocodes between cities with population up and below 30k in 2022\n",
    "    g_up, g_low = split_geocodes(geocodes)    \n",
    "\n",
    "    # get the data of each city with population above 30k\n",
    "    list_data_ep = []\n",
    "\n",
    "    for g in g_up:\n",
    "\n",
    "        list_data_ep.append(transform_data(df_ep, g))\n",
    "    \n",
    "    # get the total weekly cases of this health macroregion \n",
    "    data_macro_ep = df_ep[['casos_est']].resample('W-SUN').sum()#.agg({'casos_est':np.sum, \n",
    "                                                      #'p_rt1': np.mean, \n",
    "                                                      #'Rt': np.mean})\n",
    "\n",
    "    data_macro_ep.columns = data_macro_ep.columns + f'_{macro}'\n",
    "\n",
    "    list_data_ep.append(data_macro_ep)\n",
    "    \n",
    "    \n",
    "    # aggregate the data from small cities\n",
    "    \n",
    "    data_small_cities = df_ep.loc[df_ep.municipio_geocodigo.isin(g_low)][['casos_est','p_rt1', 'Rt']].resample('W-SUN').agg({'casos_est':np.sum, \n",
    "                                                                                                        'p_rt1': np.mean, 'Rt': np.mean})\n",
    "    data_small_cities.columns = data_small_cities.columns + f'_small'\n",
    "    \n",
    "    list_data_ep.append(data_small_cities)\n",
    "    \n",
    "    data_ep = pd.concat(list_data_ep, axis=1, join='outer')#.fillna(method='ffill')\n",
    "    \n",
    "    #remove columns with all values nan \n",
    "    data_ep = data_ep.dropna(axis =1, how = 'all')\n",
    "    \n",
    "    return data_ep \n",
    "\n",
    "\n",
    "def predictors_clim_macro(macro):\n",
    "    '''\n",
    "    This function is used to organize in a table the climate predictors related to a specific health macroregion\n",
    "    \n",
    "    :params macro: int. A four digit number\n",
    "    '''\n",
    "    geocodes, state = get_geocodes_and_state(macro)\n",
    "    \n",
    "    # get climate factors \n",
    "    df_clim = pd.read_parquet(f'../data/climate/{state}_climate.parquet',\n",
    "                         columns = predictors_clim.append('geocodigo'))\n",
    "    \n",
    "    # select only the geocodes include in the health macroregion\n",
    "    df_clim = df_clim.loc[df_clim.geocodigo.isin(geocodes)]\n",
    "\n",
    "    df_clim = df_clim.loc[df_clim.index.year >= 2010]\n",
    "\n",
    "    del df_clim['index']\n",
    "    \n",
    "    # compute other climate features \n",
    "    df_clim['temp_mean'] = (df_clim.temp_max+df_clim.temp_min)/2\n",
    "\n",
    "    df_clim['pressao_mean'] = (df_clim.pressao_max+df_clim.pressao_min)/2\n",
    "\n",
    "    df_clim['umid_mean'] = (df_clim.umid_max+df_clim.umid_min)/2\n",
    "\n",
    "    df_clim['temp_amp'] = df_clim.temp_max-df_clim.temp_min\n",
    "        # Rainy days\n",
    "    df_clim['rainy_days'] = df_clim.precip_max > 0\n",
    "        # Humidity amplitude\n",
    "    df_clim['umid_amp'] = df_clim.umid_max - df_clim.umid_min\n",
    "\n",
    "    # agg data by weekly since that's the time scale of the cases \n",
    "    df_clim = df_clim.groupby('geocodigo').resample('W-SUN').agg({'temp_min':np.mean, 'temp_max': np.mean,\n",
    "\n",
    "                                                                'umid_min':np.mean, 'umid_max': np.mean,\n",
    "                                                                'pressao_min':np.mean, 'pressao_max': np.mean,\n",
    "                                                                'precip_tot':np.sum, 'rainy_days': np.sum,\n",
    "                                                                'temp_mean':np.mean, 'temp_amp':np.mean,\n",
    "                                                                'umid_mean': np.mean,'umid_amp': np.mean,\n",
    "                                                                'pressao_mean':np.mean}).reset_index().set_index('date')\n",
    "    \n",
    "    # split the geocodes between cities with population up and below 30k in 2022\n",
    "    g_up, g_low = split_geocodes(geocodes)\n",
    "    \n",
    "    \n",
    "    # get the predictors of each city with population above 30k\n",
    "    \n",
    "    list_data_clim = []\n",
    "\n",
    "    for g in g_up:\n",
    "\n",
    "        list_data_clim.append(transform_data(df_clim, g, 'geocodigo'))\n",
    "\n",
    "    #del df_clim['geocodigo']\n",
    "\n",
    "    #data_macro_clim = df_clim.resample('W-SUN').agg({'temp_min':np.mean, 'temp_max': np.mean,\n",
    "\n",
    "     #                                                           'umid_min':np.mean, 'umid_max': np.mean,\n",
    "      #                                                          'pressao_min':np.mean, 'pressao_max': np.mean,\n",
    "       #                                                         'precip_tot':np.sum, 'rainy_days': np.sum,\n",
    "        #                                                        'temp_mean':np.mean, 'temp_amp':np.mean,\n",
    "         #                                                       'umid_mean': np.mean,'umid_amp': np.mean,\n",
    "          #                                                      'pressao_mean':np.mean}).reset_index().set_index('date')\n",
    "\n",
    "    #data_macro_clim.columns = data_macro_clim.columns + f'_{macro}'\n",
    "\n",
    "    #list_data_clim.append(data_macro_clim)\n",
    "    \n",
    "    # aggregate the data from small cities and save the mean as predictor\n",
    "    \n",
    "    data_small_cities = df_clim.loc[df_clim.geocodigo.isin(g_low)][['temp_min','temp_max',\n",
    "                                                                'umid_min', 'umid_max',\n",
    "                                                                'pressao_min', 'pressao_max',\n",
    "                                                                'precip_tot', 'rainy_days',\n",
    "                                                                'temp_mean', 'temp_amp',\n",
    "                                                                'umid_mean', 'umid_amp',\n",
    "                                                                'pressao_mean']].resample('W-SUN').mean()\n",
    "    \n",
    "\n",
    "    data_small_cities.columns = data_small_cities.columns + f'_small'\n",
    "    \n",
    "    list_data_clim.append(data_small_cities)\n",
    "    \n",
    "    data_clim = pd.concat(list_data_clim, axis=1, join='outer').fillna(method='ffill')\n",
    "    \n",
    "    #remove columns with all values nan \n",
    "    data_clim = data_clim.dropna(axis =1, how = 'all')\n",
    "    \n",
    "    return data_clim \n",
    "\n",
    "\n",
    "def get_data_macro(macro):\n",
    "    '''\n",
    "    This function is used to organize in a table the climate and epidemiological predictors \n",
    "    related to a specific health macroregion.\n",
    "    \n",
    "    :params macro: int. A four-digit number\n",
    "    '''\n",
    "    \n",
    "    data_ep = predictors_ep_macro(macro)\n",
    "    \n",
    "    data_clim = predictors_clim_macro(macro)\n",
    "    \n",
    "    data_full = pd.concat([data_ep, data_clim], axis = 1, join = 'outer')#.fillna(method = 'ffill')\n",
    "    \n",
    "    data_full = add_new_columns(data_full)\n",
    "    \n",
    "    return data_full\n",
    "\n",
    "\n",
    "def predictors_ep_state(state): \n",
    "    \n",
    "    '''\n",
    "    This function is used to organize in a table the epidemiological predictors related to a specific state    \n",
    "    :params state: str. Two leters code \n",
    "    '''\n",
    "        \n",
    "    \n",
    "    # get epidemiological factors \n",
    "    df_ep = pd.read_parquet(f'../data/cases/{state}_dengue.parquet',\n",
    "                           columns = ['data_iniSE', 'casos_est', 'municipio_geocodigo', 'p_rt1', 'Rt'])\n",
    "\n",
    "    df_ep = df_ep.sort_index()\n",
    "    \n",
    "    # this copy will be used to compute the target for all the state later \n",
    "    df_ep_copy = df_ep.copy()\n",
    "    \n",
    "    # link the geocode and the health macroregion code \n",
    "    df_ep = df_ep.reset_index().merge(dfs[['code_macro', 'geocode']].rename(columns = {'geocode':'municipio_geocodigo'}),\n",
    "                          on = 'municipio_geocodigo').set_index('data_iniSE')\n",
    "    \n",
    "    del df_ep['municipio_geocodigo']\n",
    "    \n",
    "    # resample the data based of the macroregion \n",
    "    df_ep = df_ep.groupby('code_macro').resample('W-SUN').agg({'casos_est':np.sum, \n",
    "                               'p_rt1': np.mean, \n",
    "                               'Rt': np.mean}).reset_index().set_index('data_iniSE')\n",
    "    \n",
    "    df_ep.index = pd.to_datetime(df_ep.index)\n",
    "    \n",
    "    # transform in column the data of each predictor by macroregion\n",
    "    list_data_ep = []\n",
    "\n",
    "    for m in df_ep.code_macro.unique():\n",
    "\n",
    "        list_data_ep.append(transform_data(df_ep, m, 'code_macro'))\n",
    "    \n",
    "    # get the total weekly cases of the state (it will be used as target)\n",
    "    data_state_ep = df_ep_copy[['casos_est']].resample('W-SUN').sum()#agg({'casos':np.sum, \n",
    "                               #'p_rt1': np.mean, \n",
    "                               #'Rt': np.mean})\n",
    "\n",
    "    data_state_ep.columns = data_state_ep.columns + f'_{state}'\n",
    "\n",
    "    list_data_ep.append(data_state_ep)\n",
    "    \n",
    "    # final dataframe\n",
    "    data_ep = pd.concat(list_data_ep, axis=1, join='outer')#.fillna(method='ffill')\n",
    "    \n",
    "    #remove columns with all values nan \n",
    "    data_ep = data_ep.dropna(axis =1, how = 'all')\n",
    "    \n",
    "    return data_ep \n",
    "\n",
    "def predictors_clim_state(state):\n",
    "    '''\n",
    "    This function is used to organize in a table the climate predictors related to a specific state    \n",
    "    :params state: str. Two leters code \n",
    "    '''\n",
    "    \n",
    "    # get climate factors \n",
    "    df_clim = pd.read_parquet(f'../data/climate/{state}_climate.parquet',\n",
    "                         columns = predictors_clim.append('geocodigo'))\n",
    "\n",
    "    df_clim = df_clim.loc[df_clim.index.year >= 2010]\n",
    "    \n",
    "    del df_clim['index']\n",
    "\n",
    "    df_clim['temp_mean'] = (df_clim.temp_max+df_clim.temp_min)/2\n",
    "\n",
    "    df_clim['pressao_mean'] = (df_clim.pressao_max+df_clim.pressao_min)/2\n",
    "\n",
    "    df_clim['umid_mean'] = (df_clim.umid_max+df_clim.umid_min)/2\n",
    "\n",
    "    df_clim['temp_amp'] = df_clim.temp_max-df_clim.temp_min\n",
    "        # Rainy days\n",
    "    df_clim['rainy_days'] = df_clim.precip_max > 0\n",
    "        # Humidity amplitude\n",
    "    df_clim['umid_amp'] = df_clim.umid_max - df_clim.umid_min\n",
    "\n",
    "    # link the geocode and the health macroregion code \n",
    "    \n",
    "    df_clim = df_clim.reset_index().merge(dfs[['code_macro', 'geocode']].rename(columns = {'geocode':'geocodigo'}),\n",
    "                          on = 'geocodigo').set_index('date')\n",
    "\n",
    "    del df_clim['geocodigo']\n",
    "\n",
    "    # resample the data based of the macroregion \n",
    "    df_clim = df_clim.groupby('code_macro').resample('W-SUN').agg({'temp_min':np.mean, 'temp_max': np.mean,\n",
    "\n",
    "                                                                'umid_min':np.mean, 'umid_max': np.mean,\n",
    "                                                                'pressao_min':np.mean, 'pressao_max': np.mean,\n",
    "                                                                'precip_tot':np.sum, 'rainy_days': np.sum,\n",
    "                                                                'temp_mean':np.mean, 'temp_amp':np.mean,\n",
    "                                                                'umid_mean': np.mean,'umid_amp': np.mean,\n",
    "                                                                'pressao_mean':np.mean}).reset_index().set_index('date')\n",
    "\n",
    "    # transform in column the data of each predictor by macroregion\n",
    "    list_data_clim = []\n",
    "\n",
    "    for m in df_clim.code_macro.unique():\n",
    "\n",
    "        list_data_clim.append(transform_data(df_clim, m, 'code_macro'))\n",
    "    \n",
    "\n",
    "    #data_state_clim = df_clim_copy.resample('W-SUN').agg({'temp_min':np.mean, 'temp_max': np.mean,\n",
    "\n",
    "     #                                                           'umid_min':np.mean, 'umid_max': np.mean,\n",
    "      #                                                          'pressao_min':np.mean, 'pressao_max': np.mean,\n",
    "       #                                                         'precip_tot':np.sum, 'rainy_days': np.sum,\n",
    "        #                                                        'temp_mean':np.mean, 'temp_amp':np.mean,\n",
    "         #                                                       'umid_mean': np.mean,'umid_amp': np.mean,\n",
    "          #                                                      'pressao_mean':np.mean}).reset_index().set_index('date')\n",
    "\n",
    "    #data_state_clim.columns = data_state_clim.columns + f'_{state}'\n",
    "\n",
    "    #list_data_clim.append(data_state_clim)\n",
    "\n",
    "    # final dataframe\n",
    "    data_clim = pd.concat(list_data_clim, axis=1, join='outer').ffill()#.fillna(method='ffill')\n",
    "    \n",
    "    #remove columns with all values nan \n",
    "    data_clim = data_clim.dropna(axis =1, how = 'all')\n",
    "    \n",
    "    return data_clim \n",
    "\n",
    "\n",
    "def get_data_state(state):\n",
    "    '''\n",
    "    This function is used to organize in a table the climate and epidemiological predictors \n",
    "    related to a specific state.\n",
    "    \n",
    "    :params macro: int. A four digit number\n",
    "    '''\n",
    "    \n",
    "    data_ep = predictors_ep_state(state)\n",
    "    \n",
    "    data_clim = predictors_clim_state(state)\n",
    "    \n",
    "    data_full = pd.concat([data_ep, data_clim], axis = 1, join = 'outer')#.fillna(method = 'ffill')\n",
    "    \n",
    "    data_full = add_new_columns(data_full)\n",
    "    \n",
    "    return data_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c739fe7",
   "metadata": {},
   "source": [
    "Get data for all macro in MG: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21ecaf2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T13:42:35.957276063Z",
     "start_time": "2024-01-20T13:35:53.242369790Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# for macro in dfs.loc[dfs.state=='MG'].code_macro.unique():\n",
    "for macro in dfs.code_macro.unique():\n",
    "\n",
    "    df1 = get_data_macro(macro)\n",
    "    \n",
    "    df1.to_csv(f'../data/dengue_{macro}.csv.gz')\n",
    "    \n",
    "    df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8877a987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T17:02:11.252848940Z",
     "start_time": "2024-01-19T17:02:05.396872608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casos_est_3101</th>\n",
       "      <th>p_rt1_3101</th>\n",
       "      <th>Rt_3101</th>\n",
       "      <th>casos_est_3102</th>\n",
       "      <th>p_rt1_3102</th>\n",
       "      <th>Rt_3102</th>\n",
       "      <th>casos_est_3103</th>\n",
       "      <th>p_rt1_3103</th>\n",
       "      <th>Rt_3103</th>\n",
       "      <th>casos_est_3104</th>\n",
       "      <th>...</th>\n",
       "      <th>diff_casos_est_3106</th>\n",
       "      <th>diff_casos_est_3107</th>\n",
       "      <th>diff_casos_est_3108</th>\n",
       "      <th>diff_casos_est_3109</th>\n",
       "      <th>diff_casos_est_3110</th>\n",
       "      <th>diff_casos_est_3111</th>\n",
       "      <th>diff_casos_est_3112</th>\n",
       "      <th>diff_casos_est_3113</th>\n",
       "      <th>diff_casos_est_3114</th>\n",
       "      <th>diff_casos_est_MG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>892.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>935.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-17</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-24</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2298.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-44.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>984.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            casos_est_3101  p_rt1_3101  Rt_3101  casos_est_3102  p_rt1_3102  \\\n",
       "2010-01-03            12.0         0.0      0.0             4.0         0.0   \n",
       "2010-01-10            10.0         0.0      0.0             3.0         0.0   \n",
       "2010-01-17            28.0         0.0      0.0             6.0         0.0   \n",
       "2010-01-24            55.0         0.0      0.0             6.0         0.0   \n",
       "2010-01-31            74.0         0.0      0.0             2.0         0.0   \n",
       "\n",
       "            Rt_3102  casos_est_3103  p_rt1_3103  Rt_3103  casos_est_3104  ...  \\\n",
       "2010-01-03      0.0           614.0         0.0      0.0            74.0  ...   \n",
       "2010-01-10      0.0           892.0         0.0      0.0            78.0  ...   \n",
       "2010-01-17      0.0          1092.0         0.0      0.0            85.0  ...   \n",
       "2010-01-24      0.0          1673.0         0.0      0.0            84.0  ...   \n",
       "2010-01-31      0.0          2298.0         0.0      0.0            34.0  ...   \n",
       "\n",
       "            diff_casos_est_3106  diff_casos_est_3107  diff_casos_est_3108  \\\n",
       "2010-01-03                  NaN                  NaN                  NaN   \n",
       "2010-01-10                 56.0                103.0                211.0   \n",
       "2010-01-17                 50.0                -26.0                284.0   \n",
       "2010-01-24                 14.0                  8.0                571.0   \n",
       "2010-01-31                 22.0                 70.0                -65.0   \n",
       "\n",
       "            diff_casos_est_3109  diff_casos_est_3110  diff_casos_est_3111  \\\n",
       "2010-01-03                  NaN                  NaN                  NaN   \n",
       "2010-01-10                 61.0                  3.0                 44.0   \n",
       "2010-01-17                 63.0                 -7.0                 34.0   \n",
       "2010-01-24                147.0                  4.0                 42.0   \n",
       "2010-01-31                161.0                 20.0                 64.0   \n",
       "\n",
       "            diff_casos_est_3112  diff_casos_est_3113  diff_casos_est_3114  \\\n",
       "2010-01-03                  NaN                  NaN                  NaN   \n",
       "2010-01-10                 40.0                 69.0                  9.0   \n",
       "2010-01-17                 46.0                -20.0                 22.0   \n",
       "2010-01-24                 17.0                -32.0                  7.0   \n",
       "2010-01-31                -44.0                 59.0                  9.0   \n",
       "\n",
       "            diff_casos_est_MG  \n",
       "2010-01-03                NaN  \n",
       "2010-01-10              935.0  \n",
       "2010-01-17              927.0  \n",
       "2010-01-24             1539.0  \n",
       "2010-01-31              984.0  \n",
       "\n",
       "[5 rows x 242 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 'MG'\n",
    "\n",
    "df2 = get_data_state(state)\n",
    "\n",
    "df2.to_csv(f'../data/dengue_{state}.csv.gz')\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "706fbf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casos_est_1100049</th>\n",
       "      <th>p_rt1_1100049</th>\n",
       "      <th>Rt_1100049</th>\n",
       "      <th>p_inc100k_1100049</th>\n",
       "      <th>casos_est_1100189</th>\n",
       "      <th>p_rt1_1100189</th>\n",
       "      <th>Rt_1100189</th>\n",
       "      <th>p_inc100k_1100189</th>\n",
       "      <th>casos_est_1100288</th>\n",
       "      <th>p_rt1_1100288</th>\n",
       "      <th>...</th>\n",
       "      <th>umid_amp_small</th>\n",
       "      <th>pressao_mean_small</th>\n",
       "      <th>month</th>\n",
       "      <th>SE</th>\n",
       "      <th>diff_casos_est_1100049</th>\n",
       "      <th>diff_casos_est_1100189</th>\n",
       "      <th>diff_casos_est_1100288</th>\n",
       "      <th>diff_casos_est_1100304</th>\n",
       "      <th>diff_casos_est_1101</th>\n",
       "      <th>diff_casos_est_small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>164.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.93523</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.994</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.13686</td>\n",
       "      <td>0.994904</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            casos_est_1100049  p_rt1_1100049  Rt_1100049  p_inc100k_1100049  \\\n",
       "2010-01-03              164.0            0.0         0.0          190.93523   \n",
       "\n",
       "            casos_est_1100189  p_rt1_1100189  Rt_1100189  p_inc100k_1100189  \\\n",
       "2010-01-03               52.0            0.0         0.0            140.994   \n",
       "\n",
       "            casos_est_1100288  p_rt1_1100288  ...  umid_amp_small  \\\n",
       "2010-01-03              186.0            0.0  ...        21.13686   \n",
       "\n",
       "            pressao_mean_small  month  SE  diff_casos_est_1100049  \\\n",
       "2010-01-03            0.994904      1   1                     NaN   \n",
       "\n",
       "            diff_casos_est_1100189  diff_casos_est_1100288  \\\n",
       "2010-01-03                     NaN                     NaN   \n",
       "\n",
       "            diff_casos_est_1100304  diff_casos_est_1101  diff_casos_est_small  \n",
       "2010-01-03                     NaN                  NaN                   NaN  \n",
       "\n",
       "[1 rows x 93 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro = 1101 \n",
    "\n",
    "filename_data = f'../data/dengue_{macro}.csv.gz'\n",
    "\n",
    "df = pd.read_csv(filename_data, index_col='Unnamed: 0', nrows = 1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ef99152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59d8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
